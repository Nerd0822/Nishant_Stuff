{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT for Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) has revolutionized the way we approach natural language processing tasks, including keyword extraction. Its ability to understand context and semantics makes it particularly effective. Here’s how BERT enhances keyword extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Contextual Understanding: BERT’s bidirectional training allows it to grasp the context of words in a sentence, leading to more accurate keyword identification.\n",
    "* Fine-tuning: By fine-tuning BERT on specific datasets, we can improve its performance in extracting relevant keywords tailored to particular domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.5654e-01, -2.5815e-01, -3.0364e-01,  ..., -4.4970e-01,\n",
      "           3.1113e-02,  5.6097e-01],\n",
      "         [-1.8669e-01, -5.7941e-01,  1.1139e-04,  ..., -4.4045e-01,\n",
      "           6.6886e-01,  5.0346e-01],\n",
      "         [ 1.3786e-02, -2.7492e-01,  2.9452e-01,  ..., -5.2419e-01,\n",
      "          -2.6790e-02,  7.4223e-02],\n",
      "         ...,\n",
      "         [-6.8504e-01, -6.3286e-01,  6.4164e-01,  ..., -8.2628e-01,\n",
      "          -2.5219e-01,  1.2360e-02],\n",
      "         [-7.8816e-02, -1.0502e+00, -5.7024e-01,  ..., -6.2040e-02,\n",
      "           1.3690e-02, -7.5901e-02],\n",
      "         [ 5.3279e-01, -2.1996e-01, -1.4261e-01,  ...,  7.0739e-02,\n",
      "          -8.1289e-01, -1.1717e-01]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"C:/Users/Lenovo/Documents/Rohit_AI_ML/SummariseIT/keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"C:/Users/Lenovo/Documents/Rohit_AI_ML/SummariseIT/keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, model, tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping the results of keywords from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords:\n",
      "Keyword 1: accountability\n",
      "Summary of accountability:\n",
      "Accountability is the acknowledgment of and assumption of responsibility for actions, products, decisions, and policies. It has been central to discussions related to problems in the public sector, nonprofit, private (corporate), and individual contexts. In governance, accountability has expanded beyond the basic definition of \"being called to account for one's actions\"\n",
      "Keyword 2: meeting\n",
      "Summary of meeting:\n",
      "A meeting is when two or more people come together to discuss one or more topics, often in a formal or business setting. Meetings can be used as form of group decision-making. The term \"meeting\" may refer to a lecture (one presentation), seminar (typically several presentations, small audience, one day) or congress.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"C:/Users/Lenovo/Documents/Rohit_AI_ML/SummariseIT/keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "    # Search the web for the keyword on Wikipedia\n",
    "    search_url = f\"https://en.wikipedia.org/wiki/{keyword}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the relevant information from the search result\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    extracted_text = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        extracted_text += paragraph.get_text() + \" \"\n",
    "    extracted_text = extracted_text.strip()\n",
    "\n",
    "    # Generate a summary using BART\n",
    "    def generate_summary(text, model, tokenizer):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    # Generate summary\n",
    "    summary = generate_summary(extracted_text, bart_model, bart_tokenizer)\n",
    "    print(f\"Summary of {keyword}:\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code uses pretrained data of the BERT to generate result so giving gibrish and non meaningful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords:\n",
      "Keyword 1: software\n",
      "Summary of software:\n",
      "what is software and how does it work? We look at some of the key features of software. What do you think? Let us know in the comments below. Back to Mail Online home. back to the page you came from.\"What is software?\" is a weekly, interactive look at software.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained BART model and tokenizer for summarization\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Loading the dataset of keywords\n",
    "datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "df = pd.read_csv(datapath)\n",
    "# Flatten the dataset to create a set of valid keywords\n",
    "valid_keywords = set()\n",
    "for column in df.columns:\n",
    "    valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get the [CLS] token's embedding\n",
    "    cls_embedding = last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "    similarities = torch.matmul(last_hidden_state.squeeze(), cls_embedding)\n",
    "    \n",
    "    # Get the indices of the top-n tokens with the highest similarity\n",
    "    top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "    # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "    keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Read and process input text\n",
    "file_path = \"C:/Users/Lenovo/Documents/Rohit_AI_ML/SummariseIT/keywords.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Use the function to extract keywords\n",
    "keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "# Print extracted keywords\n",
    "print(\"Extracted keywords:\")\n",
    "for idx, keyword in enumerate(keywords, start=1):\n",
    "    print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "    # Generate summary using the extracted keywords\n",
    "    summary = generate_summary(f\"what is {keyword}\", bart_model, bart_tokenizer)\n",
    "    print(f\"Summary of {keyword}:\")\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
