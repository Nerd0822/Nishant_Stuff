{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module 1,2,3 combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will record continuos audio and transcribe them it the set of 10 second audio clips until an stop event occure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyaudio\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwave\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspeech_recognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel, BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Parameters\n",
    "FORMAT = pyaudio.paInt16  # Audio format\n",
    "CHANNELS = 1  # Number of channels\n",
    "RATE = 44100  # Sample rate (Hz)\n",
    "CHUNK = 1024  # Chunk size (number of frames per buffer)\n",
    "RECORD_SECONDS = 10  # Duration of the recording (seconds)\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"  # Output filename\n",
    "\n",
    "try:\n",
    "    # Initialize PyAudio\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                        rate=RATE, input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Record data\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"Audio recorded and saved as {OUTPUT_FILENAME}\")\n",
    "\n",
    "    # Speech recognition\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(OUTPUT_FILENAME) as source:\n",
    "        audio_data = r.record(source)  # Read the entire audio file\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech using Google Web Speech API\n",
    "            text = r.recognize_google(audio_data)\n",
    "            print(\"Transcription: \" + text)\n",
    "            # Append the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"a\") as f:\n",
    "                f.write(text + \"\\n\")\n",
    "            # Save the transcription to a text file\n",
    "            with open(\"transcription.txt\", \"w\") as f:\n",
    "                f.write(text)\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand the audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "\n",
    "    # Text processing\n",
    "    with open(\"transcription.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and make lowercase\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_freq = Counter(filtered_words)\n",
    "\n",
    "    # Select the top N keywords (you can adjust N as needed)\n",
    "    N = 10\n",
    "    keywords = word_freq.most_common(N)\n",
    "\n",
    "    # Print the keywords\n",
    "    print(\"Top keywords:\")\n",
    "    for keyword, freq in keywords:\n",
    "        print(f\"{keyword}: {freq}\")\n",
    "\n",
    "    # Save the keywords to a text file\n",
    "    with open(\"keywords.txt\", \"w\") as file:\n",
    "        for keyword, freq in keywords:\n",
    "            file.write(f\"{keyword}\\n\")\n",
    "\n",
    "    # Process keywords with BERT\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Loading the dataset of keywords\n",
    "    datapath = r\"C:\\Users\\Lenovo\\Documents\\Rohit_AI_ML\\SummariseIT\\dataset.csv\"\n",
    "    df = pd.read_csv(datapath)\n",
    "    # Flatten the dataset to create a set of valid keywords\n",
    "    valid_keywords = set()\n",
    "    for column in df.columns:\n",
    "        valid_keywords.update(df[column].dropna().str.strip().tolist())\n",
    "\n",
    "    def extract_keywords_from_tokens(text, model, tokenizer, num_keywords=5):\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Get the [CLS] token's embedding\n",
    "        cls_embedding = last_hidden_states[:, 0, :].squeeze()\n",
    "        \n",
    "        # Calculate similarity between each token embedding and the [CLS] embedding\n",
    "        similarities = torch.matmul(last_hidden_states.squeeze(), cls_embedding)\n",
    "        \n",
    "        # Get the indices of the top-n tokens with the highest similarity\n",
    "        top_indices = similarities.topk(num_keywords).indices\n",
    "\n",
    "        # Extract the corresponding tokens, excluding [CLS] and checking if they are in valid_keywords\n",
    "        keywords = [tokens[i] for i in top_indices if tokens[i] != '[CLS]' and tokens[i] in valid_keywords]\n",
    "        \n",
    "        return keywords\n",
    "\n",
    "    # Read and process input text\n",
    "    file_path = \"keywords.txt\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Use the function to extract keywords\n",
    "    extracted_keywords = extract_keywords_from_tokens(text, bert_model, bert_tokenizer)\n",
    "\n",
    "    # Print extracted keywords\n",
    "    print(\"Extracted keywords:\")\n",
    "    for idx, keyword in enumerate(extracted_keywords, start=1):\n",
    "        print(f\"Keyword {idx}: {keyword}\")\n",
    "\n",
    "        # Search the web for the keyword on Wikipedia\n",
    "        search_url = f\"https://en.wikipedia.org/wiki/{keyword}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract the relevant information from the search result\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        extracted_text = \"\"\n",
    "        for paragraph in paragraphs:\n",
    "            extracted_text += paragraph.get_text() + \" \"\n",
    "        extracted_text = extracted_text.strip()\n",
    "\n",
    "        # Generate a summary using BART\n",
    "        bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "        bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "        def generate_summary(text, model, tokenizer):\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=1024)\n",
    "            summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
    "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            return summary\n",
    "\n",
    "        # Generate summary\n",
    "        summary = generate_summary(extracted_text, bart_model, bart_tokenizer)\n",
    "        print(f\"Summary of {keyword}:\")\n",
    "        print(summary)\n",
    "\n",
    "except OSError as e:\n",
    "    print(f\"OSError encountered: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
